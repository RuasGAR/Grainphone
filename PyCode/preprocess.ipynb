{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulting the International Phonetics Alphabet (IPA) recommended resources, we've reached to the selection of a very well documented dataset, called Buckeye Speech Corpus (https://buckeyecorpus.osu.edu/). It consists of a collection of speeches, which are composed by nearly 300,000 words delivered by 40 different, English language speakers. For testing and scopus purpose, we're going to use only 10 of them for the moment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is well suited for our main tasks also because it comes with a detailed and time-labeled phonetic transcription, which means we can use the timestamps to cut the raw audio exactly on the phonemes borders. \n",
    "Since the comes in fragments - and also with more data that, in our case, is not that relevant - we'll need to create ourselves a structured dataframe, that can be used to achieve both of our goals: to select a sound based on the symbolic phonetics obtained by user's standard-text inputs; and to train a Transformer Architecture so it can recognize our own voices' phonetics specifications, and separate it as expected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering our needs, we'll be build the following table-structure dataframe:\n",
    "\n",
    "ID  | Phone (written-form)      | Filepath    | Speaker          | MFCC          | ToBeDecided Audio Features \n",
    "--- | ------------------------- | ----------- | ---------------  | ------------- |\n",
    "Int | String, Categorical       | String      | Int, Categorical | Structure     |\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions (Text2Phone Part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering one speaker, we need to do the following for each of the available recordings:\n",
    "- Go through the phone descriptors file, collecting it's beginning (and subsequent ending) time, also with the phonetics themselves;\n",
    "- Use this intermediate information to slice the raw-speech-archive in actual individual files;\n",
    "- Create the dataset structure, which will lead to a better DataFrame modelling and organization as well;\n",
    "- Given a phone speech fragment, we should be able to compute its MFCC (and other features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and create/extend phonetic dataframe given a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sounds or recording issues were signaled with specific symbols, which we should ignore\n",
    "RAW_PATH = os.path.join(\"..\", \"Dataset\", \"Buckeye\", \"Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Given a single phones filepath, should map all the information into a intermediary DataFrame \"\"\"\n",
    "\n",
    "def build_one_file_df(path) -> pd.DataFrame:\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"phone\",\"recording_label\", \"start_time\", \"duration\"])\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        recording_label = lines[0].split()[1].rstrip('.sd')\n",
    "\n",
    "        lines = lines[9:len(lines)-1] # where the records starts and ends, always;\n",
    "\n",
    "        for j,l in enumerate(lines, start=1):\n",
    "            \n",
    "            start_time, _ ,phone = l.strip().split()\n",
    "            start_time = float(start_time)\n",
    "\n",
    "            next_index = df.index.max() + 1\n",
    "\n",
    "            if j == 1:\n",
    "                row = [phone, recording_label, start_time, 0]\n",
    "                df.loc[j] = row\n",
    "            elif j == len(lines)-1:\n",
    "                prev_duration = start_time - df.loc[df.index[-1], 'start_time']\n",
    "                df.loc[df.index[-1], 'duration'] = prev_duration\n",
    "                break\n",
    "            else:\n",
    "                # asserts the duration of the previous phone\n",
    "                prev_duration = start_time - df.loc[df.index[-1], 'start_time']\n",
    "                df.loc[df.index[-1], 'duration'] = prev_duration\n",
    "                row = [phone, recording_label, start_time, 0]\n",
    "                df.loc[next_index] = row\n",
    "            \n",
    "    return df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone</th>\n",
       "      <th>recording_label</th>\n",
       "      <th>start_time</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{B_TRANS}</td>\n",
       "      <td>s0101</td>\n",
       "      <td>0.102385</td>\n",
       "      <td>4.173359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SIL</td>\n",
       "      <td>s0101</td>\n",
       "      <td>4.275744</td>\n",
       "      <td>4.238019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOISE</td>\n",
       "      <td>s0101</td>\n",
       "      <td>8.513763</td>\n",
       "      <td>23.702812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IVER</td>\n",
       "      <td>s0101</td>\n",
       "      <td>32.216575</td>\n",
       "      <td>0.160018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>k</td>\n",
       "      <td>s0101</td>\n",
       "      <td>32.376593</td>\n",
       "      <td>0.245452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       phone recording_label  start_time   duration\n",
       "1  {B_TRANS}           s0101    0.102385   4.173359\n",
       "2        SIL           s0101    4.275744   4.238019\n",
       "3      NOISE           s0101    8.513763  23.702812\n",
       "4       IVER           s0101   32.216575   0.160018\n",
       "5          k           s0101   32.376593   0.245452"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function simple test\n",
    "test_raw_path = os.path.join(RAW_PATH,\"s01\",\"s0101a.phones\")\n",
    "test_df = build_one_file_df(path=test_raw_path)\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning disposable phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPOSABLE_PHONES = ['{B_TRANS}', 'SIL', 'NOISE', 'IVER', 'VOCNOISE', '<EXCLUDE-name>','LAUGH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df_phones(df:pd.DataFrame):\n",
    "    \n",
    "    indexes_to_remove = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"phone\"] in DISPOSABLE_PHONES:\n",
    "            indexes_to_remove.append(index)\n",
    "    \n",
    "    df = df.drop(indexes_to_remove)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_df = clean_df_phones(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating basic structural functions (filetrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESTINATION_BASE_PATH = os.path.join(\"..\", \"Dataset\",\"Buckeye\", \"Processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories_if_none(phones):\n",
    "    for p in phones:\n",
    "        folder_path = os.path.join(DESTINATION_BASE_PATH, p)\n",
    "        if os.path.isdir(folder_path) == False:\n",
    "            os.mkdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "create_directories_if_none(test_df[\"phone\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting sound in phonetic pieces given individual audio source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from typing import List,Tuple\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    \n",
    "    Given one dataframe and a raw audio file, it should slice the audio accordingly in multiple smaller segments. \n",
    "    Their name will follow the pattern: <PHONE>_<SPEAKER-NUM>_<RECORDING_NUM>_<HASHKEY>, and each will be saved \n",
    "    in its matching phone folder.\n",
    "    They need a unique identifier (because a single recording can have multiple entries of the same phoneme), so \n",
    "    to be easier, I've used timestamps and a random num to prevent from matching the same numbers (although technically\n",
    "    it doesn't avoid, but for now, it's more than enough)\n",
    "    The list of the saved locations are then returned so can be added in the main Dataframe\n",
    "    \n",
    "    REMEMBER TO SEE IF THEY HAVE THE SAME BIT RATE AND SAMPLE RATE  \n",
    "\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "def apply_phonetic_slicing(df:pd.DataFrame,audio_raw_path:str) -> List[str]:\n",
    "\n",
    "    recording_label = df.iat[1,df.columns.get_loc(\"recording_label\")]\n",
    "\n",
    "    # Also have to select based on the audio file (recording label)\n",
    "    audio = AudioSegment.from_file(audio_raw_path, format=\"wav\")\n",
    "    timestamps = zip(df[\"start_time\"].values,df[\"duration\"].values)\n",
    "    phones = df[\"phone\"].values\n",
    "\n",
    "    path_names = []\n",
    "\n",
    "    for i, (start_time,duration) in enumerate(timestamps):\n",
    "        \n",
    "        # the segments cuts need to be in milisseconds\n",
    "        end_time = (start_time + duration)*1000\n",
    "        segment = audio[start_time*1000:end_time]\n",
    "\n",
    "        phone_destination_folder = os.path.join(DESTINATION_BASE_PATH, phones[i])\n",
    "        audio_clip_path = os.path.join(phone_destination_folder, f\"{phones[i]}_{recording_label}_{time.time()-random.uniform(0,1):3f}\")\n",
    "\n",
    "        path_names.append(audio_clip_path)\n",
    "        segment.export(f\"{audio_clip_path}.wav\", format=\"wav\")\n",
    "\n",
    "    return path_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_paths = apply_phonetic_slicing(test_df,os.path.join(\"..\",\"Dataset\",\"Buckeye\",\"Raw\",\"s01\",\"s0101a.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text2Phone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the scripts to indeed build the main database for the program. As outputs, we expect to have a Pandas Dataframe containing audio clip locations grouped by phones - which, themselves, will also be separated in folders to better organization and ease of modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df = pd.DataFrame(columns=[\"phone\",\"speaker\",\"recording_label\",\"audio_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting all phone and audio sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support nested-dictionary structure will be created here, in order to ease the process iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKERS = [f\"s0{i}\" if i != 10 else f\"s{i}\" for i in range(1,11)]\n",
    "\n",
    "PHONE_AUDIO_REFS = {}\n",
    "for s in SPEAKERS:\n",
    "    raw_speaker_path = os.path.join(RAW_PATH, s) \n",
    "    wav_files, phone_files = [],[]\n",
    "\n",
    "    for filename in os.listdir(raw_speaker_path):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            wav_files.append(filename)\n",
    "\n",
    "        elif filename.endswith(\".phones\"):\n",
    "            phone_files.append(filename)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    PHONE_AUDIO_REFS[s] = tuple([phone_files,wav_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing everything and adding data to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AUDIO_FILES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[167], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m speaker \u001b[38;5;129;01min\u001b[39;00m SPEAKERS:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m audio_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mAUDIO_FILES\u001b[49m[speaker]:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;66;03m# CUT EACH AUDIO FILE \u001b[39;00m\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# NAME THE SAVING PLACE ACCORDINGLY\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# FIT THE DATA (THE NEW WAV PATH IN THE DATAFRAME, MAYBE NEW COLUMN)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m\"\u001b[39m, speaker, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeaker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00maudio_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m         apply_phonetic_slicing() \u001b[38;5;66;03m# proly should return this to use towards the dataframe update (or create a wrapper to call both)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AUDIO_FILES' is not defined"
     ]
    }
   ],
   "source": [
    "for speaker in SPEAKERS:\n",
    "    for audio_id in AUDIO_FILES[speaker]:\n",
    "        # CUT EACH AUDIO FILE \n",
    "        # NAME THE SAVING PLACE ACCORDINGLY\n",
    "        # FIT THE DATA (THE NEW WAV PATH IN THE DATAFRAME, MAYBE NEW COLUMN)\n",
    "        path = os.path.join(\"Base\", speaker, f\"{speaker}{audio_id}.wav\")\n",
    "        apply_phonetic_slicing() # proly should return this to use towards the dataframe update (or create a wrapper to call both)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
