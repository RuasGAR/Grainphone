{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulting the International Phonetics Alphabet (IPA) recommended resources, we've reached to the selection of a very well documented dataset, called Buckeye Speech Corpus (https://buckeyecorpus.osu.edu/). It consists of a collection of speeches, which are composed by nearly 300,000 words delivered by 40 different, English language speakers. For testing and scopus purpose, we're going to use only 10 of them for the moment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is well suited for our main tasks also because it comes with a detailed and time-labeled phonetic transcription, which means we can use the timestamps to cut the raw audio exactly on the phonemes borders. \n",
    "Since the comes in fragments - and also with more data that, in our case, is not that relevant - we'll need to create ourselves a structured dataframe, that can be used to achieve both of our goals: to select a sound based on the symbolic phonetics obtained by user's standard-text inputs; and to train a Transformer Architecture so it can recognize our own voices' phonetics specifications, and separate it as expected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering our needs, we'll be build the following table-structure dataframe:\n",
    "\n",
    "ID  | Phone (written-form)      | Filepath    | Speaker          | MFCC          | ToBeDecided Audio Features \n",
    "--- | ------------------------- | ----------- | ---------------  | ------------- |\n",
    "Int | String, Categorical       | String      | Int, Categorical | Structure     |\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions (Text2Phone Part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering one speaker, we need to do the following for each of the available recordings:\n",
    "- Go through the phone descriptors file, collecting it's beginning (and subsequent ending) time, also with the phonetics themselves;\n",
    "- Use this intermediate information to slice the raw-speech-archive in actual individual files;\n",
    "- Create the dataset structure, which will lead to a better DataFrame modelling and organization as well;\n",
    "- Given a phone speech fragment, we should be able to compute its MFCC (and other features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and create/extend phonetic dataframe given a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sounds or recording issues were signaled with specific symbols, which we should ignore\n",
    "DISPOSABLE_PHONES = ['{B_TRANS}', 'SIL', 'NOISE', 'IVER', 'VOCNOISE', '<EXCLUDE-name>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_phones_and_timestamps(path:str) -> pd.DataFrame:\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"phone\",\"recording_label\", \"start_time\", \"duration\"])\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        recording_label = lines[0].split()[1].rstrip('.sd')\n",
    "\n",
    "        lines = lines[9:len(lines)-1] # where the records starts and ends, always;\n",
    "\n",
    "        for index,l in enumerate(lines):\n",
    "            \n",
    "            start_time, _ ,phone = l.strip().split()\n",
    "            start_time = float(start_time)\n",
    "\n",
    "            if index == 0:\n",
    "                row = [phone, recording_label, start_time, 0]\n",
    "                df.loc[len(df.index)] = row\n",
    "            elif index == len(lines)-1:\n",
    "                prev_duration = start_time - df.loc[df.index[-1], 'start_time']\n",
    "                df.loc[df.index[-1], 'duration'] = prev_duration\n",
    "                break\n",
    "            else:\n",
    "                # asserts the duration of the previous phone\n",
    "                prev_duration = start_time - df.loc[df.index[-1], 'start_time']\n",
    "                df.loc[df.index[-1], 'duration'] = prev_duration\n",
    "                row = [phone, recording_label, start_time, 0]\n",
    "            \n",
    "    return df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\Dataset\\\\Raw\\\\s01\\\\s0101a.phones'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_phones_and_timestamps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDataset\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mRaw\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms01\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms0101a.phones\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mcollect_phones_and_timestamps\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_phones_and_timestamps\u001b[39m(path:\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m      3\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecording_label\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m         lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      8\u001b[0m         recording_label \u001b[38;5;241m=\u001b[39m lines[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.sd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\Desktop\\FEUP\\SSMD\\Trabalho Final\\Grainphone\\virtual_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\Dataset\\\\Raw\\\\s01\\\\s0101a.phones'"
     ]
    }
   ],
   "source": [
    "new = collect_phones_and_timestamps(path=\"..\\Dataset\\Raw\\s01\\s0101a.phones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Has to remove the \"PROIHIBITED THINGS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting sound in phonetic pieces given individual audio source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from typing import List, Tuple\n",
    "\n",
    "DESTINATION_BASE_PATH = \"../Processed\"\n",
    "#PHONE_DIRS = { key:[] for key in phones_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories() -> None:\n",
    "    for phone in PHONE_DIRS.keys():\n",
    "        os.mkdir(os.path.join(DESTINATION_BASE_PATH, phone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Given a single file name and a list of tuples containing start and endtime, it should slice the audio\n",
    "    accordingly in multiple smaller segments. They are saved under the same folder name, put in \"Processed\" directory.\n",
    "    Their name will follow the pattern: <PHONE>_<SPEAKER-NUM>_<RECORDING_NUM>\n",
    "    \n",
    "    REMEMBER TO SEE IF THEY HAVE THE SAME BIT RATE AND SAMPLE RATE  \n",
    "\n",
    " \"\"\"\n",
    "\n",
    "def extract_segments_generator(audio, timestamps):\n",
    "    for start_time, end_time in timestamps:\n",
    "        yield audio[start_time:end_time]\n",
    "\n",
    "\n",
    "def apply_phonetic_slicing(audio_file:str, df:pd.DataFrame) -> None:\n",
    "\n",
    "    # Also have to select based on the audio file (recording label)\n",
    "    audio = AudioSegment.from_file(audio_file, format=\"wav\")\n",
    "    timestamps = zip(df.loc[\"start_time\"],df.loc[\"end_time\"]) # STILL HAVE TO SELECT ONLY THE AUDIO FILE ROWS!!!\n",
    "    phones = df.loc[\"phone\"] # THE SAME THING MISSING\n",
    "\n",
    "    # think of a better way of traversal, since we will need every single information to name the file properly\n",
    "\n",
    "    for i, segment in enumerate(extract_segments_generator(audio, timestamps)):\n",
    "        \n",
    "        phone_specific_folder = PHONES_FOLDERS[phone_list[i]]\n",
    "\n",
    "        segment.export(os.path.join(DESTINATION_BASE_PATH, phone_specific_folder, f\"<PHONE>_<SPEAKER-NUM>_<RECORDING_NUM>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text2Phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating over all audio sources to get phonetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2750792505.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    SPEAKERS = {f\"s0{i}\":{} if i != 10 else f\"s{i}\": for i in range(1,11)]\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "SPEAKERS = [f\"s0{i}\" if i != 10 else f\"s{i}\" for i in range(1,11)]\n",
    "# Check the number of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
