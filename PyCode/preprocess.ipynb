{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulting the International Phonetics Alphabet (IPA) recommended resources, we've reached to the selection of a very well documented dataset, called Buckeye Speech Corpus (https://buckeyecorpus.osu.edu/). It consists of a collection of speeches, which are composed by nearly 300,000 words delivered by 40 different, English language speakers. For testing and scopus purpose, we're going to use only 10 of them for the moment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is well suited for our main tasks also because it comes with a detailed and time-labeled phonetic transcription, which means we can use the timestamps to cut the raw audio exactly on the phonemes borders. \n",
    "Since the comes in fragments - and also with more data that, in our case, is not that relevant - we'll need to create ourselves a structured dataframe, that can be used to achieve both of our goals: to select a sound based on the symbolic phonetics obtained by user's standard-text inputs; and to train a Transformer Architecture so it can recognize our own voices' phonetics specifications, and separate it as expected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering our needs, we'll be build the following table-structure dataframe:\n",
    "\n",
    "ID  | Phone (written-form)      | Filepath    | Speaker          | MFCC          | ToBeDecided Audio Features \n",
    "--- | ------------------------- | ----------- | ---------------  | ------------- |\n",
    "Int | String, Categorical       | String      | Int, Categorical | Structure     |\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions (Text2Phone Part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering one speaker, we need to do the following for each of the available recordings:\n",
    "- Go through the phone descriptors file, collecting it's beginning (and subsequent ending) time, also with the phonetics themselves;\n",
    "- Use this intermediate information to slice the raw-speech-archive in actual individual files;\n",
    "- Create the dataset structure, which will lead to a better DataFrame modelling and organization as well;\n",
    "- Given a phone speech fragment, we should be able to compute its MFCC (and other features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and create/extend phonetic dataframe given a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sounds or recording issues were signaled with specific symbols, which we should ignore\n",
    "RAW_PATH = os.path.join(\"..\", \"Dataset\", \"Buckeye\", \"Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Given a single phones filepath, should map all the information into a intermediary DataFrame \"\"\"\n",
    "\n",
    "def build_one_file_df(path) -> pd.DataFrame:\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"phone\",\"recording_label\", \"start_time\", \"duration\"])\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        \n",
    "        recording_label = lines[0].split()[1].rstrip('.sd')\n",
    "\n",
    "        lines = lines[9:len(lines)-1] # where the records starts and ends, always;\n",
    "\n",
    "        for j,l in enumerate(lines, start=1):\n",
    "            \n",
    "            try:\n",
    "                start_time, _ ,phone = l.strip().split()\n",
    "            except ValueError:\n",
    "                print(j, \"======\",l)\n",
    "\n",
    "            start_time = float(start_time)\n",
    "\n",
    "            next_index = df.index.max() + 1\n",
    "\n",
    "            if j == 1:\n",
    "                row = [phone, recording_label, start_time, 0]\n",
    "                df.loc[j] = row\n",
    "            elif j == len(lines)-1:\n",
    "                prev_duration = start_time - df.loc[df.index[-1], 'start_time']\n",
    "                df.loc[df.index[-1], 'duration'] = prev_duration\n",
    "                break\n",
    "            else:\n",
    "                # asserts the duration of the previous phone\n",
    "                prev_duration = start_time - df.loc[df.index[-1], 'start_time']\n",
    "                df.loc[df.index[-1], 'duration'] = prev_duration\n",
    "                row = [phone, recording_label, start_time, 0]\n",
    "                df.loc[next_index] = row\n",
    "            \n",
    "    return df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test_raw_path = os.path.join(RAW_PATH,\"s01\",\"s0101a.phones\")\\ntest_df = build_one_file_df(path=test_raw_path)\\ntest_df.head(5) '"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function simple test\n",
    "\"\"\" test_raw_path = os.path.join(RAW_PATH,\"s01\",\"s0101a.phones\")\n",
    "test_df = build_one_file_df(path=test_raw_path)\n",
    "test_df.head(5) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning disposable phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPOSABLE_PHONES = ['{B_TRANS}','{E_TRANS}', 'SIL', 'NOISE', 'IVER', 'VOCNOISE', '<EXCLUDE-name>','LAUGH','UNKNOWN','<exclude-Name>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df_phones(df:pd.DataFrame):\n",
    "    \n",
    "    indexes_to_remove = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"phone\"] in DISPOSABLE_PHONES:\n",
    "            indexes_to_remove.append(index)\n",
    "    \n",
    "    df = df.drop(indexes_to_remove)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Test\\ntest_df = clean_df_phones(test_df) '"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Test\n",
    "test_df = clean_df_phones(test_df) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating basic structural functions (filetrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESTINATION_BASE_PATH = os.path.join(\"..\", \"Dataset\",\"Buckeye\", \"Processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories_if_none(phones):\n",
    "    for p in phones:\n",
    "        folder_path = os.path.join(DESTINATION_BASE_PATH, p)\n",
    "        if os.path.isdir(folder_path) == False:\n",
    "            os.mkdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Test\\ncreate_directories_if_none(test_df[\"phone\"]) '"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Test\n",
    "create_directories_if_none(test_df[\"phone\"]) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting sound in phonetic pieces given individual audio source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from typing import List,Tuple\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    \n",
    "    Given one dataframe and a raw audio file, it should slice the audio accordingly in multiple smaller segments. \n",
    "    Their name will follow the pattern: <PHONE>_<SPEAKER-NUM>_<RECORDING_NUM>_<HASHKEY>, and each will be saved \n",
    "    in its matching phone folder.\n",
    "    They need a unique identifier (because a single recording can have multiple entries of the same phoneme), so \n",
    "    to be easier, I've used timestamps and a random num to prevent from matching the same numbers (although technically\n",
    "    it doesn't avoid, but for now, it's more than enough)\n",
    "    The list of the saved locations are then returned so can be added in the main Dataframe\n",
    "    \n",
    "    REMEMBER TO SEE IF THEY HAVE THE SAME BIT RATE AND SAMPLE RATE  \n",
    "\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "def apply_phonetic_slicing(df:pd.DataFrame,audio_raw_path:str) -> List[str]:\n",
    "\n",
    "    recording_label = df.iat[1,df.columns.get_loc(\"recording_label\")]\n",
    "\n",
    "    # Also have to select based on the audio file (recording label)\n",
    "    audio = AudioSegment.from_file(audio_raw_path, format=\"wav\")\n",
    "    timestamps = zip(df[\"start_time\"].values,df[\"duration\"].values)\n",
    "    phones = df[\"phone\"].values\n",
    "\n",
    "    path_names = []\n",
    "\n",
    "    for i, (start_time,duration) in enumerate(timestamps):\n",
    "        \n",
    "        # the segments cuts need to be in milisseconds\n",
    "        end_time = (start_time + duration)*1000\n",
    "        segment = audio[start_time*1000:end_time]\n",
    "\n",
    "        phone_destination_folder = os.path.join(DESTINATION_BASE_PATH, phones[i])\n",
    "        audio_seg_path = f\"{phones[i]}_{recording_label}_{time.time()-random.uniform(0,1):3f}.wav\"\n",
    "        audio_clip_destination = os.path.join(phone_destination_folder, audio_seg_path)\n",
    "        audio_database_path_name = os.path.join(phones[i],audio_seg_path)\n",
    "\n",
    "        path_names.append(audio_database_path_name)\n",
    "        segment.export(audio_clip_destination, format=\"wav\")\n",
    "\n",
    "    return path_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Test\\ntest_paths = apply_phonetic_slicing(test_df,os.path.join(\"..\",\"Dataset\",\"Buckeye\",\"Raw\",\"s01\",\"s0101a.wav\")) '"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Test\n",
    "test_paths = apply_phonetic_slicing(test_df,os.path.join(\"..\",\"Dataset\",\"Buckeye\",\"Raw\",\"s01\",\"s0101a.wav\")) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test_paths '"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" test_paths \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregating the wav file locations to the resulting dataframe of this individual process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_result_wav_path(df:pd.DataFrame, wav_paths:List[str])->pd.DataFrame:\n",
    "    df['wav_path'] = wav_paths\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Test\\ntest_df = concatenate_result_wav_path(test_df, test_paths) '"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Test\n",
    "test_df = concatenate_result_wav_path(test_df, test_paths) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text2Phone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the scripts to indeed build the main database for the program. As outputs, we expect to have a Pandas Dataframe containing audio clip locations grouped by phones - which, themselves, will also be separated in folders to better organization and ease of modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df = pd.DataFrame(columns=[\"phone\",\"recording_label\"])\n",
    "# the other columns (start_time, duration and wav_path) are attached during the preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting all phone and audio sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support nested-dictionary structure will be created here, in order to ease the process iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKERS = [f\"s0{i}\" if i != 10 else f\"s{i}\" for i in range(1,11)]\n",
    "\n",
    "PHONE_AUDIO_REFS = {}\n",
    "for s in SPEAKERS:\n",
    "    raw_speaker_path = os.path.join(RAW_PATH, s) \n",
    "    wav_files, phone_files = [],[]\n",
    "\n",
    "    for filename in os.listdir(raw_speaker_path):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            wav_files.append(filename)\n",
    "\n",
    "        elif filename.endswith(\".phones\"):\n",
    "            phone_files.append(filename)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    PHONE_AUDIO_REFS[s] = tuple([phone_files,wav_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s01': (['s0101a.phones', 's0101b.phones', 's0102a.phones', 's0102b.phones', 's0103a.phones'], ['s0101a.wav', 's0101b.wav', 's0102a.wav', 's0102b.wav', 's0103a.wav']), 's02': (['s0201a.phones', 's0201b.phones', 's0202a.phones', 's0202b.phones', 's0203a.phones', 's0203b.phones', 's0204a.phones', 's0204b.phones', 's0205a.phones', 's0205b.phones', 's0206a.phones'], ['s0201a.wav', 's0201b.wav', 's0202a.wav', 's0202b.wav', 's0203a.wav', 's0203b.wav', 's0204a.wav', 's0204b.wav', 's0205a.wav', 's0205b.wav', 's0206a.wav']), 's03': (['s0301a.phones', 's0301b.phones', 's0302a.phones', 's0302b.phones', 's0303a.phones', 's0303b.phones', 's0304a.phones', 's0304b.phones', 's0305a.phones', 's0305b.phones', 's0306a.phones'], ['s0301a.wav', 's0301b.wav', 's0302a.wav', 's0302b.wav', 's0303a.wav', 's0303b.wav', 's0304a.wav', 's0304b.wav', 's0305a.wav', 's0305b.wav', 's0306a.wav']), 's04': (['s0401a.phones', 's0401b.phones', 's0402a.phones', 's0402b.phones', 's0403a.phones', 's0403b.phones', 's0404a.phones'], ['s0401a.wav', 's0401b.wav', 's0402a.wav', 's0402b.wav', 's0403a.wav', 's0403b.wav', 's0404a.wav']), 's05': (['s0501a.phones', 's0501b.phones', 's0502a.phones', 's0502b.phones', 's0503a.phones', 's0503b.phones', 's0504a.phones'], ['s0501a.wav', 's0501b.wav', 's0502a.wav', 's0502b.wav', 's0503a.wav', 's0503b.wav', 's0504a.wav']), 's06': (['s0601a.phones', 's0601b.phones', 's0602a.phones', 's0602b.phones', 's0603a.phones'], ['s0601a.wav', 's0601b.wav', 's0602a.wav', 's0602b.wav', 's0603a.wav']), 's07': (['s0701a.phones', 's0701b.phones', 's0702a.phones', 's0702b.phones', 's0703a.phones', 's0703b.phones', 's0704a.phones'], ['s0701a.wav', 's0701b.wav', 's0702a.wav', 's0702b.wav', 's0703a.wav', 's0703b.wav', 's0704a.wav']), 's08': (['s0801a.phones', 's0801b.phones', 's0802a.phones', 's0802b.phones', 's0803a.phones', 's0803b.phones', 's0804a.phones'], ['s0801a.wav', 's0801b.wav', 's0802a.wav', 's0802b.wav', 's0803a.wav', 's0803b.wav', 's0804a.wav']), 's09': (['s0901a.phones', 's0901b.phones', 's0902a.phones', 's0902b.phones', 's0903a.phones', 's0903b.phones'], ['s0901a.wav', 's0901b.wav', 's0902a.wav', 's0902b.wav', 's0903a.wav', 's0903b.wav']), 's10': (['s1001a.phones', 's1001b.phones', 's1002a.phones', 's1002b.phones', 's1003a.phones', 's1003b.phones', 's1004a.phones'], ['s1001a.wav', 's1001b.wav', 's1002a.wav', 's1002b.wav', 's1003a.wav', 's1003b.wav', 's1004a.wav'])}\n"
     ]
    }
   ],
   "source": [
    "print(PHONE_AUDIO_REFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script for processing all of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s0101a.phones\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s0101b.phones\n",
      "s0102a.phones\n",
      "s0102b.phones\n",
      "s0103a.phones\n",
      "s0201a.phones\n",
      "s0201b.phones\n",
      "s0202a.phones\n",
      "s0202b.phones\n",
      "s0203a.phones\n",
      "s0203b.phones\n",
      "s0204a.phones\n",
      "s0204b.phones\n",
      "s0205a.phones\n",
      "s0205b.phones\n",
      "s0206a.phones\n",
      "s0301a.phones\n",
      "s0301b.phones\n",
      "s0302a.phones\n",
      "s0302b.phones\n",
      "s0303a.phones\n",
      "s0303b.phones\n",
      "s0304a.phones\n",
      "s0304b.phones\n",
      "s0305a.phones\n",
      "s0305b.phones\n",
      "s0306a.phones\n",
      "s0401a.phones\n",
      "s0401b.phones\n",
      "s0402a.phones\n",
      "s0402b.phones\n",
      "s0403a.phones\n",
      "s0403b.phones\n",
      "s0404a.phones\n",
      "s0501a.phones\n",
      "s0501b.phones\n",
      "s0502a.phones\n",
      "s0502b.phones\n",
      "s0503a.phones\n",
      "s0503b.phones\n",
      "790 ======   203.462129  122 ae ; h\n",
      "\n",
      "s0504a.phones\n",
      "s0601a.phones\n",
      "s0601b.phones\n",
      "s0602a.phones\n",
      "s0602b.phones\n",
      "s0603a.phones\n",
      "s0701a.phones\n",
      "s0701b.phones\n",
      "s0702a.phones\n",
      "s0702b.phones\n",
      "s0703a.phones\n",
      "s0703b.phones\n",
      "s0704a.phones\n",
      "s0801a.phones\n",
      "s0801b.phones\n",
      "s0802a.phones\n",
      "s0802b.phones\n",
      "s0803a.phones\n",
      "s0803b.phones\n",
      "s0804a.phones\n",
      "s0901a.phones\n",
      "s0901b.phones\n",
      "s0902a.phones\n",
      "s0902b.phones\n",
      "s0903a.phones\n",
      "s0903b.phones\n",
      "s1001a.phones\n",
      "s1001b.phones\n",
      "s1002a.phones\n",
      "5257 ======   553.687827  122 n ; m\n",
      "\n",
      "s1002b.phones\n",
      "s1003a.phones\n",
      "s1003b.phones\n",
      "s1004a.phones\n"
     ]
    }
   ],
   "source": [
    "for speaker,(phone_files, wav_files) in PHONE_AUDIO_REFS.items():\n",
    "    for i in range(0,len(wav_files)):\n",
    "        \n",
    "        phone_f = phone_files[i]\n",
    "        print(phone_f)\n",
    "        wav_f = wav_files[i]\n",
    "        \n",
    "        local_base_path = os.path.join(RAW_PATH, speaker)\n",
    "        phone_whole_path = os.path.join(RAW_PATH, speaker, phone_f)\n",
    "        wav_whole_path = os.path.join(RAW_PATH, speaker, wav_f)\n",
    "\n",
    "        local_df = build_one_file_df(phone_whole_path)\n",
    "        local_df = clean_df_phones(local_df)\n",
    "        create_directories_if_none(local_df[\"phone\"])\n",
    "        wav_clips_paths = apply_phonetic_slicing(local_df,wav_whole_path)\n",
    "        local_final_df = concatenate_result_wav_path(local_df, wav_clips_paths)\n",
    "\n",
    "        prod_df = pd.concat([prod_df, local_df], ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the application idea is to recover random samples of a given phone, it would be interesting to our dataset to be sorted in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df = prod_df.sort_values(by=[\"phone\", \"recording_label\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df.to_csv(\"production_data.csv\",sep=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
